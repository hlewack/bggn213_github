---
title: "class8_unsupervisedmachinelearning"
format: pdf
editor: visual
author: Hannah Lewack (A69034788)
---

side note before we start, why is scaling important for PCAs

```{r}
mtcars
head(mtcars)
apply(mtcars, 2, mean)
apply(mtcars, 2, sd)
pca<-prcomp(mtcars)
biplot(pca)

#now lets try scaling
mtscale<-scale(mtcars)
head(mtscale)
round(apply(mtscale, 2, mean), 3)
round(apply(mtscale, 2, sd), 3)

#lets plot
library(ggplot2)
ggplot(mtcars)+aes(mpg,disp)+geom_point()
ggplot(mtscale)+aes(mpg,disp)+geom_point()
pca2<-prcomp(mtscale)
biplot(pca2)
```

```{r prepping/exploring breast cancer FNA data}
library(dplyr)
# Save your input data file into your Project directory
fna.data <- "WisconsinCancer.csv"
# Complete the following code to input the data and store as wisc.df
wisc.df <- read.csv(fna.data, row.names=1)
head(wisc.df)
diagnosis <-as.factor(wisc.df$diagnosis)
# We can use -1 here to remove the first column
wisc.data <- wisc.df[,-1]


#Q1. How many observations are in this dataset? 569
nrow(wisc.data)
#Q2. How many of the observations have a malignant diagnosis?
malignentnumber<-wisc.data%>%filter(diagnosis=="M")
nrow(malignentnumber)
#Q3. How many variables/features in the data are suffixed with _mean? 10
colnames(wisc.data)
length(grep("_mean",colnames(wisc.data)))
```

```{r PCA breast cancer FNA data}
# Check column means and standard deviations
colMeans(wisc.data)
apply(wisc.data,2,sd)
wisc.pr <- prcomp(wisc.data, scale=T)
summary(wisc.pr)

#Q4. From your results, what proportion of the original variance is captured by the first principal components (PC1)? #44%

#Q5. How many principal components (PCs) are required to describe at least 70% of the original variance in the data? #Up to PC3

#Q6. How many principal components (PCs) are required to describe at least 90% of the original variance in the data? #up to PC8

biplot(wisc.pr)

#Q7. What stands out to you about this plot? Is it easy or difficult to understand? Why?
#this plot looks terrible it is impossible to read because there are so many values on it
```

```{r fixing PCA plot}
# Scatter plot observations by components 1 and 2
wisc.pr$x
plot(wisc.pr$x, col = diagnosis , 
     xlab = "PC1", ylab = "PC2")
# Repeat for components 1 and 3
plot(wisc.pr$x[,1:3], col = diagnosis, 
     xlab = "PC1", ylab = "PC3")

# Create a data.frame for ggplot
df <- as.data.frame(wisc.pr$x)
df$diagnosis <- diagnosis
# Load the ggplot2 package
library(ggplot2)
# Make a scatter plot colored by diagnosis
ggplot(df) + 
  aes(PC1, PC2, col=diagnosis) + 
  geom_point()
```
```{r PCA varience}
pr.var <- wisc.pr$sdev^2
head(pr.var)
# Variance explained by each principal component: pve
pve <- (wisc.pr$sdev^2) / sum(pr.var)

# Plot variance explained for each principal component
plot(pve, xlab = "Principal Component", 
     ylab = "Proportion of Variance Explained", 
     ylim = c(0, 1), type = "o")
# Alternative scree plot of the same data, note data driven y-axis
barplot(pve, ylab = "Precent of Variance Explained",
     names.arg=paste0("PC",1:length(pve)), las=2, axes = FALSE)
axis(2, at=pve, labels=round(pve,2)*100 )
## ggplot based graph
#install.packages("factoextra")
library(factoextra)
fviz_eig(wisc.pr, addlabels = TRUE)

#Q9. For the first principal component, what is the component of the loading vector (i.e. wisc.pr$rotation[,1]) for the feature concave.points_mean? This tells us how much this original feature contributes to the first PC.
#it contributes 44.3 percent to the first PC
```
```{r hierarchical clustering}
# Scale the wisc.data data using the "scale()" function
data.scaled <- scale(wisc.data)
data.dist <- dist(data.scaled)
wisc.hclust <- hclust(data.dist,"complete")
#Q10. Using the plot() and abline() functions, what is the height at which the clustering model has 4 clusters? #19
plot(wisc.hclust)
abline(h=19, col="red", lty=2)
wisc.hclust.clusters <- cutree(wisc.hclust, h=19)
table(wisc.hclust.clusters)
table(wisc.hclust.clusters, diagnosis)
#Q11. OPTIONAL: Can you find a better cluster vs diagnoses match by cutting into a different number of clusters between 2 and 10? How do you judge the quality of your result in each case?
wisc.hclust.clusters2 <- cutree(wisc.hclust, h=8)
table(wisc.hclust.clusters2, diagnosis)
#looking at the amount of trees with few numbers is a good indicator of quality
wisc.pr.hclust <- hclust(data.dist,method="ward.D2")
#Q12. Which method gives your favorite results for the same data.dist dataset? Explain your reasoning.
#my favorite mode of clustering is single because I appreaciate the chaos and I find it funny but I can acknowledge that it's not the prettiest and think other functions are probably better realistically
```

```{r clustering on PCA results}
grps <- cutree(wisc.pr.hclust, k=2)
table(grps)
table(grps, diagnosis)
plot(wisc.pr$x[,1:2], col=grps)
plot(wisc.pr$x[,1:2], col=diagnosis)
g <- as.factor(grps)
levels(g)
g <- relevel(g,2)
levels(g)
plot(wisc.pr$x[,1:2], col=g)
```
```{r PCA sensitivity}
## Use the distance along the first 7 PCs for clustering i.e. wisc.pr$x[, 1:7]
wisc.pr.hclust <- hclust(dist(wisc.pr$x[, 1:7]), method="ward.D2")
wisc.pr.hclust.clusters <- cutree(wisc.pr.hclust, k=2)
table(wisc.pr.hclust.clusters, diagnosis)
#Q13. How well does the newly created model with four clusters separate out the two diagnoses?
#It does well, it separates them out into disease and non-disease
#barry said to skip 13-15
```
```{r}
#url <- "new_samples.csv"
url <- "https://tinyurl.com/new-samples-CSV"
new <- read.csv(url)
npc <- predict(wisc.pr, newdata=new)
npc
plot(wisc.pr$x[,1:2], col=g)
points(npc[,1], npc[,2], col="blue", pch=16, cex=3)
text(npc[,1], npc[,2], c(1,2), col="white")
#Q16. Which of these new patients should we prioritize for follow up based on your results?
#I would want to investigate patient 2 because it follows the pattern of malignent cells
```




